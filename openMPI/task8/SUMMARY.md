# MPI Task 8 - Итоговая сводка

## Выполненное задание

**Задание 8 из MPI**: Модифицировать программу из задания 3, используя операцию одновременного выполнения передачи и приема данных. Сравнить результаты вычислительных экспериментов.

## Реализация

### Ключевое изменение

**Task 3** использовал раздельные операции:
```cpp
MPI_Send(send_buf, n, MPI_BYTE, dest, tag, comm);
MPI_Recv(recv_buf, n, MPI_BYTE, source, tag, comm, status);
```

**Task 8** использует комбинированную операцию:
```cpp
MPI_Sendrecv(send_buf, n, MPI_BYTE, dest, sendtag,
             recv_buf, n, MPI_BYTE, source, recvtag,
             comm, status);
```

### Преимущества MPI_Sendrecv

1. **Безопасность**: Автоматически предотвращает deadlock
2. **Простота**: Один вызов вместо двух
3. **Производительность**: MPI может оптимизировать одновременную передачу
4. **Управление буферами**: MPI сам управляет внутренними буферами

## Структура проекта

```
openMPI/task8/
├── main.cpp                    # Основная программа с MPI_Sendrecv
├── Makefile                    # Компиляция проекта
├── job.sh                      # SLURM скрипт (Fixed - 2 ноды)
├── job_auto.sh                 # SLURM скрипт (Auto - планировщик)
├── plot_graphs.py              # Визуализация результатов
├── README.md                   # Полная документация
├── QUICKSTART.md               # Быстрый старт
├── SUMMARY.md                  # Этот файл
├── data.csv                    # Результаты Fixed (создается при запуске)
├── data_auto.csv               # Результаты Auto (создается при запуске)
└── graphs/                     # Графики (создаются при визуализации)
```

## Конфигурации экспериментов

### 1. Fixed (job.sh)
- **Параметры**: `--nodes=2 --ntasks-per-node=1`
- **Размещение**: Процессы гарантированно на разных узлах
- **Коммуникация**: Через сеть (межузловая)
- **Цель**: Реалистичный сценарий распределенных вычислений

### 2. Auto (job_auto.sh)
- **Параметры**: `--ntasks=2`
- **Размещение**: Планировщик решает сам
- **Коммуникация**: Зависит от размещения (shared memory или сеть)
- **Цель**: Оптимальное размещение для производительности

## Параметры эксперимента

- **Процессы**: 2 (ping-pong)
- **Размеры сообщений**: 0 байт → 16 МБ (степени двойки)
- **Итерации**: 
  - 1000 для малых сообщений (≤ 64 КБ)
  - 100 для средних (64 КБ - 1 МБ)
  - 20 для больших (> 1 МБ)

## Измеряемые метрики

1. **Время передачи (Time)**: Среднее время одной передачи в секундах
2. **Пропускная способность (Bandwidth)**: Скорость передачи в МБ/сек

## Запуск

### На кластере
```bash
cd openMPI/task8
sbatch job.sh       # Fixed
sbatch job_auto.sh  # Auto
```

### Визуализация
```bash
python3 plot_graphs.py
```

## Создаваемые графики

### Отдельные графики (4 шт):
1. **time_vs_size_fixed.png** - время передачи (Fixed)
2. **bandwidth_vs_size_fixed.png** - пропускная способность (Fixed)
3. **time_vs_size_auto.png** - время передачи (Auto)
4. **bandwidth_vs_size_auto.png** - пропускная способность (Auto)

### Сравнительные графики (4 шт):
5. **comparison_time.png** - сравнение времени Fixed vs Auto
6. **comparison_bandwidth.png** - сравнение пропускной способности
7. **comparison_latency.png** - латентность для малых сообщений
8. **comparison_speedup.png** - относительная производительность

## Ожидаемые результаты

### Производительность Fixed
- Латентность: ~100-200 мкс (сетевая коммуникация)
- Пропускная способность: ~100-110 МБ/сек (ограничена сетью)
- Стабильные результаты

### Производительность Auto
- Латентность: может быть меньше (если на одном узле)
- Пропускная способность: может быть выше (shared memory)
- Зависит от загрузки кластера

### Сравнение конфигураций
- **Speedup**: отношение времени Auto к Fixed
- **Латентность**: разница для малых сообщений
- **Пропускная способность**: максимальные значения

## Технические детали

- **Язык**: C++
- **Компилятор**: mpic++ с оптимизацией -O3
- **MPI**: OpenMPI
- **Планировщик**: SLURM
- **Время выполнения**: ~10 минут на эксперимент

## Анализ результатов

При анализе обратите внимание на:

1. **Латентность** (время для 0 байт):
   - Показывает накладные расходы MPI
   - Разница между Fixed и Auto показывает влияние типа коммуникации

2. **Пропускная способность**:
   - Растет с размером сообщения
   - Достигает максимума на больших сообщениях
   - Ограничена пропускной способностью сети (Fixed) или памяти (Auto)

3. **Сравнение конфигураций**:
   - Fixed: реалистичный сценарий для распределенных вычислений
   - Auto: оптимальное размещение для производительности
   - Speedup показывает преимущество одной конфигурации над другой

## Выводы

1. **MPI_Sendrecv** - более безопасная и удобная альтернатива паре Send/Recv
2. Производительность зависит от типа коммуникации (сеть vs shared memory)
3. Автоматическое размещение может дать лучшую производительность
4. Фиксированное размещение дает более предсказуемые результаты

## Зачем две конфигурации?

Ping-pong всегда использует ровно 2 процесса, поэтому масштабируемость не тестируется. Однако сравнение двух конфигураций показывает:

- **Влияние типа коммуникации**: сеть vs shared memory
- **Роль планировщика**: автоматическое vs ручное размещение
- **Компромисс**: производительность vs предсказуемость

## Дополнительные материалы

- [README.md](README.md) - Полная документация
- [QUICKSTART.md](QUICKSTART.md) - Быстрый старт
- [MPI_Sendrecv документация](https://www.open-mpi.org/doc/current/man3/MPI_Sendrecv.3.php)

## Соответствие заданию

✅ Модифицирована программа из задания 3  
✅ Использована операция одновременного выполнения передачи и приема (MPI_Sendrecv)  
✅ Подготовлены две конфигурации для сравнения  
✅ Создана визуализация результатов (8 графиков)  
✅ Реализовано статистическое сравнение  
✅ Создана полная документация  

**Задание выполнено полностью.**